from __future__ import annotations

import logging
import os
import re
import sys
import uuid
from pathlib import Path
from typing import Any, List, Tuple, cast

import streamlit as st
from dotenv import load_dotenv

# Ensure project root is on sys.path so that 'app' package is importable
PROJECT_ROOT = Path(__file__).resolve().parent.parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from app.auth import render_login_gate, render_sidebar_user
from app.diff_utils import unified_markdown_diff
from app.export import export_docx, export_pdf
from app.file_handler import process_uploaded_file_with_gemini
from app.llm import (
    DEFAULT_MODEL_NAME,
    bootstrap_spec,
    check_spec_completeness,
    generate_invention_description,
    generate_title,
    next_questions,
    refine_document,
    regenerate_spec,
)
from app.spec_builder import add_revision, append_assistant_message, append_user_answer
from app.state import AppState, Attachment, Idea, Revision
from app.storage import delete_idea, get_idea, load_ideas, save_ideas

APP_TITLE = "Patent Chat"
DEFAULT_CATEGORY = "é˜²ç½"
PROJECT_ROOT = Path(__file__).resolve().parent.parent

# Suppress very noisy PDF parsing warnings (e.g., advanced encodings)
logging.getLogger("pypdf").setLevel(logging.ERROR)


def _load_instruction_markdown() -> str:
    """Load drafting instruction document with fallback to sample.md."""
    primary = PROJECT_ROOT / "LLM_Prompt_for_Patent_Application_Drafting_from_Idea.md"
    fallback = PROJECT_ROOT / "sample.md"
    path = primary if primary.exists() else fallback
    try:
        return path.read_text(encoding="utf-8")
    except Exception:
        return ""


def _load_invention_instruction_markdown() -> str:
    """Load invention description instruction document with fallback to sample.md."""
    primary = PROJECT_ROOT / "LLM_Prompt_for_Invention_Explanation_Full_JP.md"
    fallback = PROJECT_ROOT / "sample.md"
    path = primary if primary.exists() else fallback
    try:
        return path.read_text(encoding="utf-8")
    except Exception:
        return ""


def _clean_ai_message(content: str) -> str:
    """AIãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰å°å…¥éƒ¨åˆ†ã‚’é™¤å¤–ã—ã¦æœ¬æ–‡ã®ã¿è¿”ã™.

    ç›®çš„:
    - LLMãŒä»˜ã‘ãŒã¡ãªå‰ç½®ãæ–‡ã‚’å‰Šã‚‹
    - ç®‡æ¡æ›¸ãã®ç•ªå·ç­‰ã¯å¤‰æ›´ã—ãªã„ï¼ˆä»–æ‰€ã§åˆ©ç”¨ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ï¼‰
    """
    if not content:
        return ""

    # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆä¸€èˆ¬çš„ãªå°å…¥è¡¨ç¾ï¼‰
    intro_patterns = [
        r'^æ‰¿çŸ¥.*?ã€‚\s*',
        r'^äº†è§£.*?ã€‚\s*',
        r'^ç¢ºèªã•ã›ã¦.*?ã€‚\s*',
        r'^ã‚ã‹ã‚Šã¾ã—ãŸ.*?ã€‚\s*',
        r'^ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™.*?ã€‚\s*',
        r'^ãã‚Œã§ã¯.*?ã€‚\s*',
        r'^ä»¥ä¸‹.*?ç¢ºèª.*?ã€‚\s*',
        r'^æ¬¡ã®ç‚¹ã«ã¤ã„ã¦.*?ã€‚\s*',
        r'^è¿½åŠ ã§ç¢ºèª.*?ã€‚\s*',
    ]

    cleaned = (content or "").strip()

    # å°å…¥éƒ¨åˆ†ã‚’é™¤å»
    for pattern in intro_patterns:
        cleaned = re.sub(pattern, '', cleaned, flags=re.MULTILINE | re.DOTALL)

    # å†’é ­ã®ç©ºè¡Œã‚’é™¤å»
    cleaned = re.sub(r'^\s*\n+', '', cleaned)

    return cleaned.strip()


def _strip_leading_list_marker(text: str) -> str:
    """å…ˆé ­ã®ç®‡æ¡æ›¸ããƒ»ç•ªå·ä»˜ã‘ãƒãƒ¼ã‚«ãƒ¼ã‚’1å›ã ã‘é™¤å»ã™ã‚‹ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£.

    ä¾‹:
    - "1. è³ªå•?" â†’ "è³ªå•?"
    - "ï¼ˆ1ï¼‰ è³ªå•?" â†’ "è³ªå•?"
    - "Q1: è³ªå•?" â†’ "è³ªå•?"
    - "- è³ªå•?" â†’ "è³ªå•?"
    """
    if not text:
        return ""
    patterns = [
        r'^\s*[\-ãƒ»â€¢]\s+',
        r'^\s*[\u2460-\u2473\u24F5-\u24FE\u2776-\u277F]\s+',
        r'^\s*[ï¼ˆ(]\s*[0-9ï¼-ï¼™]{1,3}\s*[ï¼‰)]\s*',
        r'^\s*[0-9ï¼-ï¼™]{1,3}[\.ï¼ã€]\s+',
        r'^\s*[0-9ï¼-ï¼™]{1,3}[)ï¼‰]\s+',
        r'^\s*(?:Q|ï¼±|å•)\s*[0-9ï¼-ï¼™]{1,3}[:ï¼š\.ï¼]?\s+',
    ]
    cleaned = text
    for p in patterns:
        new_cleaned = re.sub(p, '', cleaned)
        if new_cleaned != cleaned:
            cleaned = new_cleaned
            break
    return cleaned


def init_session_state() -> None:
    if "app_state" not in st.session_state:
        default_model = os.getenv("GEMINI_MODEL", DEFAULT_MODEL_NAME)
        st.session_state.app_state = AppState(gemini_model=default_model)
        os.environ["GEMINI_MODEL"] = default_model
    if "ideas" not in st.session_state:
        st.session_state.ideas = load_ideas()


def _estimate_completeness_percent(idea: Idea) -> int:
    """Return an integer 0-100 representing current completeness.

    Prefers stored LLM score when available; otherwise uses a simple heuristic
    based on placeholders and draft length.
    """
    try:
        if getattr(idea, "completeness_score", 0.0):
            return int(max(0.0, min(100.0, round(idea.completeness_score))))
        text = idea.draft_spec_markdown or ""
        has_placeholders = "æœªè¨˜è¼‰" in text
        spec_length = len(text)
        if not has_placeholders and spec_length > 3000:
            score = min(100.0, 70.0 + (spec_length - 3000) / 100.0)
        else:
            score = 50.0 if has_placeholders else 60.0
        return int(score)
    except Exception:
        return 0


def sidebar_ui():
    ideas: List[Idea] = st.session_state.ideas
    state: AppState = st.session_state.app_state

    # Always-available Home navigation (without logging out)
    if st.sidebar.button("ğŸ  ãƒˆãƒƒãƒ—ã¸æˆ»ã‚‹", use_container_width=True):
        state.selected_idea_id = None
        state.show_new_idea_form = False
        # Clear transient flags such as hearing start
        if "start_hearing" in st.session_state:
            st.session_state.pop("start_hearing", None)
        st.rerun()

    model_options = ["gemini-2.5-pro", "gemini-2.5-flash"]
    current_index = (
        model_options.index(state.gemini_model) if state.gemini_model in model_options else 0
    )
    selected_model = st.sidebar.selectbox("Geminiãƒ¢ãƒ‡ãƒ«", model_options, index=current_index)
    if selected_model != state.gemini_model:
        state.gemini_model = selected_model
        os.environ["GEMINI_MODEL"] = selected_model

    # Move the idea list title below the model selector
    st.sidebar.title("ã‚¢ã‚¤ãƒ‡ã‚¢ä¸€è¦§")

    # New idea button
    if st.sidebar.button("ï¼‹ æ–°è¦ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’ä½œæˆ", use_container_width=True):
        state.show_new_idea_form = True
        st.rerun()

    # Idea list
    for idea in ideas:
        cols = st.sidebar.columns([0.7, 0.15, 0.15])
        if cols[0].button(f"{idea.title or '(ç„¡é¡Œ)'}", key=f"sel-{idea.id}"):
            state.selected_idea_id = idea.id
            state.show_new_idea_form = False
            st.rerun()
        if cols[1].button("ç·¨é›†", key=f"edit-{idea.id}"):
            state.selected_idea_id = idea.id
            state.show_new_idea_form = True
            st.rerun()
        if cols[2].button("å‰Šé™¤", key=f"del-{idea.id}"):
            st.session_state.ideas = delete_idea(ideas, idea.id)
            save_ideas(st.session_state.ideas)
            if state.selected_idea_id == idea.id:
                state.selected_idea_id = None
            st.rerun()


def new_idea_form():
    st.subheader("æ–°è¦ã‚¢ã‚¤ãƒ‡ã‚¢")
    category = st.selectbox(
        "ã‚«ãƒ†ã‚´ãƒª", ["é˜²ç½", "åŒ»ç™‚", "è£½é€ ", "ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢", "ç’°å¢ƒ", "ãã®ä»–"], index=0
    )
    description = st.text_area(
        "ã‚¢ã‚¤ãƒ‡ã‚¢ã®è©³ç´°èª¬æ˜", height=160, placeholder="ã‚¢ã‚¤ãƒ‡ã‚¢ã®æ¦‚è¦ã‚’è¨˜è¼‰â€¦"
    )

    # File upload section
    st.markdown("### é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã®æ·»ä»˜ï¼ˆä»»æ„ï¼‰")
    uploaded_files = st.file_uploader(
        "ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ",
        accept_multiple_files=True,
        help="ãƒ†ã‚­ã‚¹ãƒˆã€PDFã€ç”»åƒã€Wordã€PowerPointãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ï¼ˆå„10MBä»¥å†…ï¼‰",
    )

    # Process uploaded files
    attachments_to_add = []
    if uploaded_files:
        for uploaded_file in uploaded_files:
            comment = st.text_input(
                f"{uploaded_file.name} ã¸ã®ã‚³ãƒ¡ãƒ³ãƒˆ",
                placeholder="ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®èª¬æ˜ã‚’å…¥åŠ›...",
                key=f"comment_{uploaded_file.name}",
            )
            attachments_to_add.append((uploaded_file, comment))

    cols = st.columns(2)
    if cols[0].button("ä¿å­˜", type="primary"):
        idea_id = str(uuid.uuid4())
        with st.status("å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™â€¦", expanded=True) as status:
            status.update(label="ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆä¸­â€¦", state="running")
            title = generate_title(description)

            # Process attachments
            status.update(label="æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ä¸­â€¦", state="running")
            attachments = []
            attachment_dicts = []
            gemini_files: List[Any] = []

            # Get Gemini client to fetch file objects
            import logging
            import os

            from google import genai

            logger = logging.getLogger("patent_chat.main")
            api_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
            gemini_client = None
            if api_key:
                try:
                    gemini_client = genai.Client(api_key=api_key)
                except Exception:
                    pass

            for uploaded_file, comment in attachments_to_add:
                try:
                    file_data = process_uploaded_file_with_gemini(uploaded_file, comment)
                    attachment = Attachment(
                        filename=file_data["filename"],
                        content_base64=file_data["content_base64"],
                        comment=file_data["comment"],
                        file_type=file_data["file_type"],
                        upload_time=file_data["upload_time"],
                        gemini_file_id=file_data.get("gemini_file_id"),
                        gemini_mime_type=file_data.get("gemini_mime_type"),
                        extracted_text=file_data.get("extracted_text"),
                    )
                    attachments.append(attachment)
                    attachment_dicts.append(
                        {
                            "filename": file_data["filename"],
                            "extracted_text": file_data["extracted_text"],
                            "comment": file_data["comment"],
                        }
                    )

                    # Add Gemini file object if available
                    if file_data.get("gemini_file_id") and gemini_client:
                        try:
                            # Get the actual file object from Gemini
                            gemini_file = gemini_client.files.get(name=file_data["gemini_file_id"])
                            gemini_files.append(gemini_file)
                        except Exception:
                            file_id = file_data['gemini_file_id']
                            logger.warning(f"Failed to get Gemini file object for {file_id}")
                    elif file_data.get("gemini_file_id"):
                        # No client available, just store the ID
                        gemini_files.append(file_data["gemini_file_id"])

                except Exception as e:
                    st.warning(f"ãƒ•ã‚¡ã‚¤ãƒ« {uploaded_file.name} ã®å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")

            status.update(label="ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’ä¿å­˜ä¸­â€¦", state="running")
            idea = Idea(
                id=idea_id,
                title=title,
                category=category,
                description=description,
                attachments=attachments,
            )
            st.session_state.ideas.append(idea)
            save_ideas(st.session_state.ideas)

            status.update(label="åˆæœŸãƒ‰ãƒ©ãƒ•ãƒˆç”Ÿæˆä¸­â€¦", state="running")
            manual_md = _load_instruction_markdown()
            spec_result, error_msg = bootstrap_spec(
                manual_md, idea.description, attachments=attachment_dicts, gemini_files=gemini_files
            )
            if error_msg:
                st.error(f"âš ï¸ {error_msg}")
                st.info("åŸºæœ¬çš„ãªéª¨æ ¼ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚å¾Œã§å†ç”Ÿæˆã‚’è©¦ã—ã¦ãã ã•ã„ã€‚")
            idea.draft_spec_markdown = spec_result
            save_ideas(st.session_state.ideas)

            # Also generate Invention Description (ç™ºæ˜èª¬æ˜æ›¸)
            status.update(label="ç™ºæ˜èª¬æ˜æ›¸ï¼ˆãƒ•ãƒ«ï¼‰ã‚’ç”Ÿæˆä¸­â€¦", state="running")
            inv_manual_md = _load_invention_instruction_markdown()
            inv_text, inv_err = generate_invention_description(
                inv_manual_md,
                title,
                idea.description,
                transcript=idea.messages,
                attachments=attachment_dicts,
                gemini_files=gemini_files,
            )
            if inv_err:
                st.warning(f"âš ï¸ ç™ºæ˜èª¬æ˜æ›¸ã®ç”Ÿæˆã§å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸ: {inv_err}")
            idea.invention_description_markdown = inv_text
            save_ideas(st.session_state.ideas)

            status.update(label="åˆå›è³ªå•ã‚’æº–å‚™ä¸­â€¦", state="running")
            qs, q_error = next_questions(
                manual_md,
                idea.messages,
                idea.draft_spec_markdown,
                num_questions=10,
                version=idea.draft_version,
                is_final=idea.is_final,
                attachments=attachment_dicts,
            )
            if q_error:
                st.warning(f"âš ï¸ è³ªå•ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {q_error}")
                st.info("ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®è³ªå•ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            for q in qs:
                append_assistant_message(idea.messages, q)
            save_ideas(st.session_state.ideas)

            status.update(label="å®Œäº†", state="complete")

        st.session_state.app_state.selected_idea_id = idea_id
        st.session_state.app_state.show_new_idea_form = False
        st.session_state.start_hearing = True
        st.rerun()
    if cols[1].button("ã‚­ãƒ£ãƒ³ã‚»ãƒ«"):
        st.session_state.app_state.show_new_idea_form = False


def edit_idea_form(idea: Idea):
    st.subheader("ã‚¢ã‚¤ãƒ‡ã‚¢ç·¨é›†")
    title = st.text_input("ã‚¿ã‚¤ãƒˆãƒ«", value=idea.title)
    categories = ["é˜²ç½", "åŒ»ç™‚", "è£½é€ ", "ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢", "ç’°å¢ƒ", "ãã®ä»–"]
    try:
        idx = categories.index(idea.category)
    except ValueError:
        idx = 0
    category = st.selectbox("ã‚«ãƒ†ã‚´ãƒª", categories, index=idx)
    description = st.text_area("ã‚¢ã‚¤ãƒ‡ã‚¢ã®è©³ç´°èª¬æ˜", value=idea.description, height=160)
    if st.button("æ›´æ–°"):
        idea.title = title
        idea.category = category
        idea.description = description
        save_ideas(st.session_state.ideas)
        st.success("æ›´æ–°ã—ã¾ã—ãŸã€‚")


def _calculate_question_start_number(idea: Idea) -> int:
    """Calculate the starting number for questions based on all previous questions."""
    # Count all assistant messages that are questions (answered or not)
    question_count = 0
    for msg in idea.messages:
        if msg.get("role") == "assistant":
            # Simple heuristic: if it contains "ï¼Ÿ" or "?", it's likely a question
            content = msg.get("content", "")
            if "ï¼Ÿ" in content or "?" in content:
                question_count += 1

    # If we're on the first version or no questions yet, start from 1
    if question_count == 0:
        return 1

    # Count only answered questions (pairs of assistant followed by user)
    answered_count = 0
    i = 0
    while i < len(idea.messages):
        # Look for assistant messages
        if i < len(idea.messages) and idea.messages[i].get("role") == "assistant":
            # Check if there's at least one user answer after this batch of questions
            j = i
            # Skip all consecutive assistant messages
            while j < len(idea.messages) and idea.messages[j].get("role") == "assistant":
                j += 1
            # Now j points to first non-assistant message or end
            # Check if there are user messages
            if j < len(idea.messages) and idea.messages[j].get("role") == "user":
                # Count the assistant messages in this batch as answered
                for k in range(i, j):
                    content = idea.messages[k].get("content", "")
                    if "ï¼Ÿ" in content or "?" in content:
                        answered_count += 1
            i = j
        else:
            i += 1

    # Start numbering from answered_count + 1
    return answered_count + 1


def _prepare_attachment_dicts(idea: Idea) -> Tuple[List[dict], List]:
    """Convert Attachment objects to dictionaries for LLM functions.

    Returns:
        Tuple of (attachment_dicts, gemini_files)
    """
    import base64

    from app.file_handler import extract_text_from_file

    attachment_dicts = []
    gemini_files: List[Any] = []

    # Get Gemini client to fetch file objects
    import logging
    import os

    from google import genai

    logger = logging.getLogger("patent_chat.main")
    api_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
    gemini_client = None
    if api_key:
        try:
            gemini_client = genai.Client(api_key=api_key)
        except Exception:
            pass

    for att in idea.attachments:
        # Check if we have a Gemini file ID
        if att.gemini_file_id and gemini_client:
            try:
                # Get the actual file object from Gemini
                gemini_file = gemini_client.files.get(name=att.gemini_file_id)
                gemini_files.append(gemini_file)
            except Exception:
                logger.warning(f"Failed to get Gemini file object for {att.gemini_file_id}")
        elif att.gemini_file_id:
            # No client available, just store the ID
            gemini_files.append(cast(Any, att.gemini_file_id))
            # Still add to dicts for backward compatibility
            attachment_dicts.append(
                {
                    "filename": att.filename,
                    "extracted_text": "",  # Will be processed by Gemini directly
                    "comment": att.comment,
                }
            )
        else:
            # No Gemini file ID: use stored extracted_text if available to avoid re-parsing
            if getattr(att, "extracted_text", None):
                attachment_dicts.append(
                    {
                        "filename": att.filename,
                        "extracted_text": att.extracted_text or "",
                        "comment": att.comment,
                    }
                )
            else:
                # Fall back to local extraction
                file_bytes = base64.b64decode(att.content_base64)
                extracted_text = extract_text_from_file(file_bytes, att.filename)
                attachment_dicts.append(
                    {
                        "filename": att.filename,
                        "extracted_text": extracted_text,
                        "comment": att.comment,
                    }
                )

    return attachment_dicts, gemini_files


def _render_hearing_section(idea: Idea, manual_md: str, show_questions_first: bool = False):
    """å…±é€šã®è³ªå•è¡¨ç¤ºãƒ­ã‚¸ãƒƒã‚¯."""
    # Hearing round equals draft version
    hearing_round = idea.draft_version

    st.subheader(f"AI ãƒ’ã‚¢ãƒªãƒ³ã‚°ï¼ˆç¬¬{hearing_round}å›ï¼‰")
    # Progress indicator (informational)
    progress = _estimate_completeness_percent(idea)
    st.caption("å®Œæˆåº¦ï¼ˆç›®æ¨™ 85% ä»¥ä¸Šã§å®Œäº†ï¼‰")
    st.progress(progress)

    # Collect consecutive assistant messages at the tail (unanswered)
    tail_assistant: list[str] = []
    for m in reversed(idea.messages):
        if m.get("role") == "assistant":
            tail_assistant.append(m.get("content", ""))
        else:
            break
    tail_assistant = list(reversed(tail_assistant))

    def _looks_like_question(text: str) -> bool:
        if not text:
            return False
        t = str(text).strip()
        # Consider as question if it:
        # - ends with a question mark, or
        # - is yes/no style, or
        # - is marked as open-ended (è‡ªç”±è¨˜è¿°)
        return t.endswith("ï¼Ÿ") or t.endswith("?") or ("ã¯ã„/ã„ã„ãˆ" in t) or ("è‡ªç”±è¨˜è¿°" in t)

    def _looks_like_yes_no_question(text: str) -> bool:
        if not text:
            return False
        t = str(text).strip()
        return "ã¯ã„/ã„ã„ãˆ" in t or "ï¼ˆã¯ã„/ã„ã„ãˆ" in t

    pending_candidates = [q for q in tail_assistant if _looks_like_question(q)]
    # De-duplicate while preserving order
    seen_q: set[str] = set()
    pending_questions: list[tuple[str, str]] = []
    for q in pending_candidates:
        if q not in seen_q:
            seen_q.add(q)
            q_type = "yesno" if _looks_like_yes_no_question(q) else "open"
            pending_questions.append((q, q_type))
    pending_questions = pending_questions[:10]

    # Determine which trailing assistant messages to hide from the history
    # (exactly those shown in the pending questions)
    to_hide_indices = set()
    match_from_end = list(reversed([q for q, _ in pending_questions]))
    ptr = 0
    for idx in range(len(idea.messages) - 1, -1, -1):
        if ptr >= len(match_from_end):
            break
        m = idea.messages[idx]
        if m.get("role") != "assistant":
            break
        if m.get("content", "") == match_from_end[ptr]:
            to_hide_indices.add(idx)
            ptr += 1

    # For version 2+, show questions first if requested
    if show_questions_first and pending_questions:
        _render_pending_questions(idea, pending_questions, manual_md)

    # Conversation history (exclude pending questions to avoid duplication)
    if any(i not in to_hide_indices for i in range(len(idea.messages))):
        # Properly pair questions and answers considering batch format
        # Filter out pending questions first
        filtered_messages = []
        for i, msg in enumerate(idea.messages):
            if i not in to_hide_indices:
                filtered_messages.append(msg)

        # Collect questions and answers in batches
        questions = []
        answers = []

        j = 0
        while j < len(filtered_messages):
            # Collect consecutive assistant messages
            batch_questions = []
            while j < len(filtered_messages) and filtered_messages[j]["role"] == "assistant":
                batch_questions.append(_clean_ai_message(filtered_messages[j]['content']))
                j += 1

            # Collect consecutive user messages
            batch_answers = []
            while j < len(filtered_messages) and filtered_messages[j]["role"] == "user":
                batch_answers.append(filtered_messages[j]['content'])
                j += 1

            # Pair this batch
            for k in range(len(batch_questions)):
                if k < len(batch_answers):
                    questions.append(batch_questions[k])
                    answers.append(batch_answers[k])

        # Display Q&A history in expander for version 2+
        if show_questions_first and questions:
            with st.expander("**ã“ã‚Œã¾ã§ã®è³ªå•ã¨å›ç­”**", expanded=False):
                # Display paired Q&A
                for q, a in zip(questions, answers):
                    st.markdown(f"{q}: {a}")
        elif not show_questions_first and questions:
            # Version 1: display inline
            for q, a in zip(questions, answers):
                st.markdown(f"{q}: {a}")

    # Pending assistant questions at tail -> per-question radios (ã¯ã„/ã„ã„ãˆ/ã‚ã‹ã‚‰ãªã„)
    if not show_questions_first and pending_questions:
        _render_pending_questions(idea, pending_questions, manual_md)


def _render_pending_questions(
    idea: Idea, pending_questions: list[tuple[str, str]] | list[str], manual_md: str
):
    """Render pending questions form."""
    st.markdown("**æœªå›ç­”ã®è³ªå•**ï¼ˆå„é …ç›®ã«å›ç­”ã—ã¦ã€Œå›ç­”ã‚’ã¾ã¨ã‚ã¦é€ä¿¡ã€ã€‚è‡ªç”±è¨˜è¿°ã¯ä»»æ„ï¼‰")
    with st.form(f"qa-form-{idea.id}"):
        selections: list[str] = []
        # Normalize input to list of tuples
        normalized: list[tuple[str, str]] = []
        for q in pending_questions:
            if isinstance(q, tuple):
                normalized.append(q)
            else:
                normalized.append((q, "yesno"))
        st.caption(f"æœªå›ç­”ã®è³ªå•: {len(normalized)}ä»¶")
        # Calculate the starting question number based on all previous questions
        start_num = _calculate_question_start_number(idea)
        for i, (q, q_type) in enumerate(normalized, start=start_num):
            cleaned_q = _strip_leading_list_marker(_clean_ai_message(q))
            st.markdown(f"Q{i}: {cleaned_q}")
            key = f"ans-{idea.id}-v{idea.draft_version}-{i}"
            if q_type == "yesno":
                choice = st.radio(
                    key=key,
                    label="å›ç­”",
                    options=["ã¯ã„", "ã„ã„ãˆ", "ã‚ã‹ã‚‰ãªã„"],
                    index=2,  # Default to "ã‚ã‹ã‚‰ãªã„"
                    horizontal=True,
                )
                selections.append(choice)
            else:
                text = st.text_area(key=key, label="å›ç­”ï¼ˆä»»æ„ï¼‰", value="")
                selections.append(text.strip() or "ç„¡å›ç­”")
        submitted = st.form_submit_button("å›ç­”ã‚’ã¾ã¨ã‚ã¦é€ä¿¡", type="primary")
        if submitted:
            for ans in selections:
                append_user_answer(idea.messages, ans)
            with st.spinner("ãƒ‰ãƒ©ãƒ•ãƒˆæ›´æ–°ä¸­â€¦"):
                attachment_dicts, gemini_files = _prepare_attachment_dicts(idea)
                spec_result, error_msg = regenerate_spec(
                    manual_md,
                    idea.description,
                    idea.messages,
                    attachments=attachment_dicts,
                    gemini_files=gemini_files,
                )
                if error_msg:
                    st.error(f"âš ï¸ {error_msg}")
                    st.info("å‰ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ä¿æŒã—ã¾ã™ã€‚")
                else:
                    idea.draft_spec_markdown = spec_result
                    idea.draft_version += 1

                # Regenerate Invention Description in parallel
                inv_text, inv_err = generate_invention_description(
                    _load_invention_instruction_markdown(),
                    idea.title,
                    idea.description,
                    transcript=idea.messages,
                    attachments=attachment_dicts,
                    gemini_files=gemini_files,
                )
                if inv_err:
                    st.warning(f"âš ï¸ ç™ºæ˜èª¬æ˜æ›¸ã®ç”Ÿæˆã§å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸ: {inv_err}")
                else:
                    idea.invention_description_markdown = inv_text

                # Check if this should be the final version based on completeness only
                is_complete, score = check_spec_completeness(
                    manual_md, idea.draft_spec_markdown, idea.draft_version
                )
                print(
                    f"DEBUG: Completeness check - is_complete={is_complete}, "
                    f"score={score}, version={idea.draft_version}"
                )
                # Store latest score for UI progress
                try:
                    idea.completeness_score = float(score)
                except Exception:
                    pass
                if is_complete:
                    idea.is_final = True
                    print(f"DEBUG: Set is_final=True due to completeness score={score}")

                save_ideas(st.session_state.ideas)

            # Generate next questions only if not final
            if not idea.is_final and not error_msg:  # Don't generate questions if error
                with st.spinner("æ¬¡ã®è³ªå•ã‚’æº–å‚™ä¸­â€¦"):
                    try:
                        attachment_dicts, gemini_files = _prepare_attachment_dicts(idea)
                        qs2, q_error = next_questions(
                            manual_md,
                            idea.messages,
                            idea.draft_spec_markdown,
                            num_questions=10,
                            version=idea.draft_version,
                            is_final=idea.is_final,
                            attachments=attachment_dicts,
                        )
                        if q_error:
                            st.warning(f"âš ï¸ è³ªå•ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {q_error}")
                            st.info("ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®è³ªå•ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                    except Exception as e:
                        print(f"ERROR: Exception in next_questions: {e}")
                        st.warning(f"âš ï¸ è³ªå•ç”Ÿæˆã§äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)[:100]}")
                        st.info("ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®è³ªå•ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                        # Provide fallback questions
                        qs2 = [
                            "ç¾è¡Œãƒ‰ãƒ©ãƒ•ãƒˆã«æœªè¨˜è¼‰ç®‡æ‰€ãŒã‚ã‚Šã¾ã™ã€‚å›³é¢ã¯å¿…è¦ã§ã™ã‹ï¼Ÿï¼ˆã¯ã„/ã„ã„ãˆï¼‰",
                            "å®Ÿæ–½ä¾‹ã¯è¤‡æ•°ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿï¼ˆã¯ã„/ã„ã„ãˆï¼‰",
                            "ç™ºæ˜ã®åŠ¹æœã«å®šé‡çš„æ ¹æ‹ ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿï¼ˆã¯ã„/ã„ã„ãˆï¼‰",
                            "æ—¢å­˜æŠ€è¡“ã¨ã®é•ã„ã‚’æ˜ç¢ºã«èª¬æ˜ã§ãã¾ã™ã‹ï¼Ÿï¼ˆã¯ã„/ã„ã„ãˆï¼‰",
                            "ã“ã®ç™ºæ˜ã®æœ€ã‚‚é‡è¦ãªåˆ©ç‚¹ã¯ä½•ã§ã™ã‹ï¼Ÿï¼ˆã¯ã„/ã„ã„ãˆï¼‰",
                            "è¿½åŠ ã®å®Ÿæ–½å½¢æ…‹ã¯å­˜åœ¨ã—ã¾ã™ã‹ï¼Ÿï¼ˆã¯ã„/ã„ã„ãˆï¼‰",
                            "å›³é¢ã®å‚ç…§ç•ªå·ã¯é©åˆ‡ã§ã™ã‹ï¼Ÿï¼ˆã¯ã„/ã„ã„ãˆï¼‰",
                            "åŠ¹æœã®è£ä»˜ã‘ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿï¼ˆã¯ã„/ã„ã„ãˆï¼‰",
                            "ã“ã®ç™ºæ˜ã®æƒ³å®šã•ã‚Œã‚‹å¿œç”¨ä¾‹ã¯ä½•ã§ã™ã‹ï¼Ÿï¼ˆè‡ªç”±è¨˜è¿°ï¼‰",
                            "ç‰¹ã«å¼·èª¿ã—ãŸã„æŠ€è¡“çš„åŠ¹æœã¯ä½•ã§ã™ã‹ï¼Ÿï¼ˆè‡ªç”±è¨˜è¿°ï¼‰",
                        ][:10]
                        q_error = "äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼"

                    print(f"DEBUG: Generated {len(qs2)} questions for version {idea.draft_version}")
                    # Always add questions even if there was an error
                    for q in qs2:
                        append_assistant_message(idea.messages, q)
                        print(f"DEBUG: Added question: {q[:50]}...")
                    save_ideas(st.session_state.ideas)
            else:
                print(
                    f"DEBUG: Skipping question generation - "
                    f"is_final={idea.is_final}, version={idea.draft_version}"
                )
            st.rerun()


def _render_refine_ui(idea: Idea) -> None:
    st.subheader("LLMä¿®æ­£ï¼ˆè‡ªç„¶æ–‡ã®ä¿®æ­£æŒ‡ç¤ºã‚’åæ˜ ï¼‰")
    with st.form(f"refine-form-{idea.id}"):
        feedback = st.text_area(
            "ä¿®æ­£æŒ‡ç¤ºã‚’å…¥åŠ›",
            placeholder=("ä¾‹: ç¬¬2ç« ã®èª²é¡Œã‚’å…·ä½“åŒ–ã—ã€åŠ¹æœã§ã¯æ ¹æ‹ ã‚’æ˜è¨˜ã€‚è«‹æ±‚é …ã¯æ–‡æœ«è¡¨ç¾ã‚’çµ±ä¸€ã€‚"),
            height=120,
        )
        target = st.radio(
            "å¯¾è±¡æ–‡æ›¸",
            options=["ç™ºæ˜èª¬æ˜æ›¸", "æ˜ç´°æ›¸ãƒ‰ãƒ©ãƒ•ãƒˆ"],
            index=0,
            horizontal=True,
        )
        submitted = st.form_submit_button("ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ä½œæˆ", type="primary")

    if submitted:
        doc_type = "explanation" if target == "ç™ºæ˜èª¬æ˜æ›¸" else "spec"
        original = (
            idea.invention_description_markdown
            if doc_type == "explanation"
            else idea.draft_spec_markdown
        )
        with st.status("ä¿®æ­£æ¡ˆã‚’ç”Ÿæˆä¸­â€¦", expanded=False):
            refined, err = refine_document(original, feedback, doc_type=doc_type)
        if err:
            st.warning(f"âš ï¸ ä¿®æ­£ç”Ÿæˆã§å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸ: {err}")
        diff = unified_markdown_diff(
            original,
            refined,
            fromfile=f"before_{doc_type}",
            tofile=f"after_{doc_type}",
        )
        st.session_state[f"refine_preview_{idea.id}"] = {
            "doc_type": doc_type,
            "feedback": feedback,
            "refined": refined,
            "diff": diff,
        }

    preview = st.session_state.get(f"refine_preview_{idea.id}")
    if preview:
        st.markdown("---")
        st.markdown("**ä¿®æ­£ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼**")
        cols = st.columns(2)
        c1 = cols[0]
        c2 = cols[1]
        with c1:
            st.caption("å·®åˆ†ï¼ˆunifiedï¼‰")
            st.code(preview["diff"] or "(å·®åˆ†ãªã—)", language="diff")
        with c2:
            st.caption("ä¿®æ­£å¾Œã®æœ¬æ–‡ï¼ˆå…¨æ–‡ï¼‰")
            st.markdown(preview["refined"] or "(å‡ºåŠ›ãªã—)")

        # Export preview without saving
        exp_cols = st.columns(2)
        suffix = "ç™ºæ˜èª¬æ˜æ›¸PRV" if preview["doc_type"] == "explanation" else "æ˜ç´°æ›¸PRV"
        name_base = f"{idea.title}_{suffix}"
        name_docx_p, data_docx_p = export_docx(name_base, preview["refined"])
        exp_cols[0].download_button(
            "ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’Wordã§ä¿å­˜",
            data=data_docx_p,
            file_name=name_docx_p,
            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
            use_container_width=True,
        )
        name_pdf_p, data_pdf_p = export_pdf(name_base, preview["refined"])
        exp_cols[1].download_button(
            "ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’PDFã§ä¿å­˜",
            data=data_pdf_p,
            file_name=name_pdf_p,
            mime="application/pdf",
            use_container_width=True,
        )

        cols2 = st.columns(2)
        col_a = cols2[0]
        col_b = cols2[1]
        if col_a.button("æ¡ç”¨ã—ã¦ä¿å­˜", type="primary"):
            doc_type = preview["doc_type"]
            text = preview["refined"]
            before = (
                idea.invention_description_markdown
                if doc_type == "explanation"
                else idea.draft_spec_markdown
            )
            if not text or text.strip() == (before or "").strip():
                st.info("å†…å®¹ã«å¤‰æ›´ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            else:
                # Apply to idea
                if doc_type == "explanation":
                    idea.invention_description_markdown = text
                else:
                    idea.draft_spec_markdown = text

                # Record revision
                import uuid as _uuid

                rev = Revision(
                    id=str(_uuid.uuid4()),
                    doc_type=doc_type,
                    feedback=preview["feedback"],
                    text=text,
                    diff=preview["diff"],
                    model=os.getenv("GEMINI_MODEL", DEFAULT_MODEL_NAME),
                    meta={
                        "from": f"{len((before or ''))} chars",
                        "to": f"{len((text or ''))} chars",
                    },
                )
                add_revision(idea, rev, max_history=50)
                save_ideas(st.session_state.ideas)
                st.success("ä¿®æ­£ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")
                # Clear preview
                st.session_state.pop(f"refine_preview_{idea.id}", None)
                st.rerun()
        if col_b.button("ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ç ´æ£„"):
            st.session_state.pop(f"refine_preview_{idea.id}", None)
            st.rerun()


def hearing_ui(idea: Idea):
    manual_md = _load_instruction_markdown()
    inv_manual_md = _load_invention_instruction_markdown()

    # File upload section for hearing
    with st.expander("è¿½åŠ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", expanded=False):
        st.markdown("ãƒ’ã‚¢ãƒªãƒ³ã‚°ä¸­ã«è¿½åŠ ã§å‚è€ƒè³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™")
        new_files = st.file_uploader(
            "ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ",
            accept_multiple_files=True,
            help="ãƒ†ã‚­ã‚¹ãƒˆã€PDFã€ç”»åƒã€Wordã€PowerPointãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ï¼ˆå„10MBä»¥å†…ï¼‰",
            key=f"hearing_upload_{idea.id}",
        )

        if new_files:
            attachments_to_add = []
            for uploaded_file in new_files:
                comment = st.text_input(
                    f"{uploaded_file.name} ã¸ã®ã‚³ãƒ¡ãƒ³ãƒˆ",
                    placeholder="ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®èª¬æ˜ã‚’å…¥åŠ›...",
                    key=f"hearing_comment_{idea.id}_{uploaded_file.name}",
                )
                attachments_to_add.append((uploaded_file, comment))

            if st.button("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ ", key=f"add_files_{idea.id}"):
                for uploaded_file, comment in attachments_to_add:
                    try:
                        file_data = process_uploaded_file_with_gemini(uploaded_file, comment)
                        attachment = Attachment(
                            filename=file_data["filename"],
                            content_base64=file_data["content_base64"],
                            comment=file_data["comment"],
                            file_type=file_data["file_type"],
                            upload_time=file_data["upload_time"],
                            gemini_file_id=file_data.get("gemini_file_id"),
                            gemini_mime_type=file_data.get("gemini_mime_type"),
                            extracted_text=file_data.get("extracted_text"),
                        )
                        idea.attachments.append(attachment)
                        st.success(f"{uploaded_file.name} ã‚’è¿½åŠ ã—ã¾ã—ãŸ")
                    except Exception as e:
                        st.error(f"ãƒ•ã‚¡ã‚¤ãƒ« {uploaded_file.name} ã®å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
                save_ideas(st.session_state.ideas)
                st.rerun()

    # Display existing attachments
    if idea.attachments:
        with st.expander(f"æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ« ({len(idea.attachments)}ä»¶)", expanded=False):
            for att in idea.attachments:
                cols = st.columns([3, 1])
                cols[0].markdown(f"ğŸ“ **{att.filename}** - {att.comment}")
                import base64

                file_bytes = base64.b64decode(att.content_base64)
                cols[1].download_button(
                    "ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=file_bytes,
                    file_name=att.filename,
                    mime=att.file_type,
                    key=f"download_{idea.id}_{att.filename}",
                )

    # Ensure draft exists
    if not idea.draft_spec_markdown:
        with st.spinner("åˆæœŸãƒ‰ãƒ©ãƒ•ãƒˆç”Ÿæˆä¸­â€¦"):
            attachment_dicts, gemini_files = _prepare_attachment_dicts(idea)
            spec_result, error_msg = bootstrap_spec(
                manual_md, idea.description, attachments=attachment_dicts, gemini_files=gemini_files
            )
            if error_msg:
                st.error(f"âš ï¸ {error_msg}")
                st.info("åŸºæœ¬çš„ãªéª¨æ ¼ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚")
            idea.draft_spec_markdown = spec_result
            save_ideas(st.session_state.ideas)

    # Ensure invention description exists
    if not idea.invention_description_markdown:
        with st.spinner("ç™ºæ˜èª¬æ˜æ›¸ï¼ˆãƒ•ãƒ«ï¼‰ã‚’åˆæœŸç”Ÿæˆä¸­â€¦"):
            attachment_dicts, gemini_files = _prepare_attachment_dicts(idea)
            inv_text, inv_err = generate_invention_description(
                inv_manual_md,
                idea.title,
                idea.description,
                transcript=idea.messages,
                attachments=attachment_dicts,
                gemini_files=gemini_files,
            )
            if inv_err:
                st.warning(f"âš ï¸ ç™ºæ˜èª¬æ˜æ›¸ã®ç”Ÿæˆã§å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸ: {inv_err}")
            idea.invention_description_markdown = inv_text
            save_ideas(st.session_state.ideas)

    # Auto-generate initial questions if none exist yet (up to 10)
    if not any(m.get("role") == "assistant" for m in idea.messages) and not idea.is_final:
        with st.spinner("åˆå›è³ªå•ã‚’æº–å‚™ä¸­â€¦"):
            attachment_dicts, gemini_files = _prepare_attachment_dicts(idea)
            qs, q_error = next_questions(
                manual_md,
                idea.messages,
                idea.draft_spec_markdown,
                num_questions=10,
                version=idea.draft_version,
                is_final=idea.is_final,
                attachments=attachment_dicts,
            )
            if q_error:
                st.warning(f"âš ï¸ è³ªå•ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {q_error}")
                st.info("ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®è³ªå•ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            for q in qs:
                append_assistant_message(idea.messages, q)
            save_ideas(st.session_state.ideas)
        st.rerun()

    # Handle final version display
    if idea.is_final:
        # Invention Description first (primary deliverable)
        st.subheader("ç™ºæ˜èª¬æ˜æ›¸ï¼ˆãƒ•ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼‰")
        st.success("âœ… ç™ºæ˜èª¬æ˜æ›¸ãŒå®Œæˆã—ã¾ã—ãŸã€‚ä»¥ä¸‹ãŒæœ€çµ‚ç‰ˆã®å†…å®¹ã§ã™ã€‚")
        st.markdown(idea.invention_description_markdown or "æœªç”Ÿæˆ", unsafe_allow_html=False)
        c3, c4 = st.columns(2)
        inv_title = f"{idea.title}_ç™ºæ˜èª¬æ˜æ›¸"
        name_docx2, data_docx2 = export_docx(inv_title, idea.invention_description_markdown)
        c3.download_button(
            "ğŸ“ ç™ºæ˜èª¬æ˜æ›¸ã‚’Wordã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=data_docx2,
            file_name=name_docx2,
            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
            use_container_width=True,
        )
        name_pdf2, data_pdf2 = export_pdf(inv_title, idea.invention_description_markdown)
        c4.download_button(
            "ğŸ“„ ç™ºæ˜èª¬æ˜æ›¸ã‚’PDFã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=data_pdf2,
            file_name=name_pdf2,
            mime="application/pdf",
            use_container_width=True,
        )

        # Reference: patent specification draft (not a submission)
        st.markdown("---")
        with st.expander("ç”Ÿæˆã•ã‚ŒãŸæ˜ç´°æ›¸ãƒ‰ãƒ©ãƒ•ãƒˆï¼ˆå‚è€ƒï¼‰", expanded=False):
            st.markdown(idea.draft_spec_markdown or "æœªç”Ÿæˆ", unsafe_allow_html=False)
            with st.expander("æ˜ç´°æ›¸ãƒ‰ãƒ©ãƒ•ãƒˆã‚’ç·¨é›†"):
                edited = st.text_area("Markdown", value=idea.draft_spec_markdown, height=500)
                if st.button("ç·¨é›†å†…å®¹ã‚’ä¿å­˜"):
                    idea.draft_spec_markdown = edited
                    save_ideas(st.session_state.ideas)
                    st.success("ä¿å­˜ã—ã¾ã—ãŸã€‚")
            st.markdown("### ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆå‚è€ƒï¼‰")
            c1, c2 = st.columns(2)
            name_docx, data_docx = export_docx(idea.title, idea.draft_spec_markdown)
            c1.download_button(
                "ğŸ“ Word ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=data_docx,
                file_name=name_docx,
                mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                use_container_width=True,
            )
            name_pdf, data_pdf = export_pdf(idea.title, idea.draft_spec_markdown)
            c2.download_button(
                "ğŸ“„ PDF ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=data_pdf,
                file_name=name_pdf,
                mime="application/pdf",
                use_container_width=True,
            )

        # Show Q&A history at the bottom
        with st.expander("è³ªç–‘å¿œç­”å±¥æ­´", expanded=False):
            # Properly pair questions and answers considering batch format
            questions = []
            answers = []

            j = 0
            while j < len(idea.messages):
                # Collect consecutive assistant messages
                batch_questions = []
                while j < len(idea.messages) and idea.messages[j]["role"] == "assistant":
                    batch_questions.append(_clean_ai_message(idea.messages[j]['content']))
                    j += 1

                # Collect consecutive user messages
                batch_answers = []
                while j < len(idea.messages) and idea.messages[j]["role"] == "user":
                    batch_answers.append(idea.messages[j]['content'])
                    j += 1

                # Pair this batch
                for k in range(len(batch_questions)):
                    if k < len(batch_answers):
                        questions.append(batch_questions[k])
                        answers.append(batch_answers[k])
                    else:
                        questions.append(batch_questions[k])
                        answers.append("(æœªå›ç­”)")

            # Display Q&A pairs (question: answer on same line)
            for q, a in zip(questions, answers):
                st.markdown(f"{q}: {a}")

    # Non-final version display
    elif idea.draft_version == 1:
        # Version 1: Questions first layout
        _render_hearing_section(idea, manual_md, show_questions_first=False)

        # Invention Description (initial draft) first
        with st.expander("ç™ºæ˜èª¬æ˜æ›¸ï¼ˆãƒ‰ãƒ©ãƒ•ãƒˆï¼‰", expanded=False):
            st.markdown(idea.invention_description_markdown or "æœªç”Ÿæˆ", unsafe_allow_html=False)

        # Draft in collapsed expander (keep v1 label for tests)
        with st.expander("ç”Ÿæˆã•ã‚ŒãŸæ˜ç´°æ›¸ãƒ‰ãƒ©ãƒ•ãƒˆï¼ˆç¬¬1ç‰ˆï¼‰", expanded=False):
            st.markdown(idea.draft_spec_markdown or "æœªç”Ÿæˆ", unsafe_allow_html=False)
        st.divider()
        _render_refine_ui(idea)

    else:
        # Version 2-4: New layout - questions first, then history, then draft
        st.subheader("AI ãƒ’ã‚¢ãƒªãƒ³ã‚°")

        # Show questions first, then Q&A history
        _render_hearing_section(idea, manual_md, show_questions_first=True)

        st.divider()

        # Invention Description (draft) expander for non-final versions first
        with st.expander("ç™ºæ˜èª¬æ˜æ›¸ï¼ˆãƒ‰ãƒ©ãƒ•ãƒˆï¼‰", expanded=False):
            st.markdown(idea.invention_description_markdown or "æœªç”Ÿæˆ", unsafe_allow_html=False)
            st.markdown("---")
            c3, c4 = st.columns(2)
            inv_title2 = f"{idea.title}_ç™ºæ˜èª¬æ˜æ›¸"
            name_docx2, data_docx2 = export_docx(inv_title2, idea.invention_description_markdown)
            c3.download_button(
                "ç™ºæ˜èª¬æ˜æ›¸ã‚’Wordã§ä¿å­˜",
                data=data_docx2,
                file_name=name_docx2,
                mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                use_container_width=True,
            )
            name_pdf2, data_pdf2 = export_pdf(inv_title2, idea.invention_description_markdown)
            c4.download_button(
                "ç™ºæ˜èª¬æ˜æ›¸ã‚’PDFã§ä¿å­˜",
                data=data_pdf2,
                file_name=name_pdf2,
                mime="application/pdf",
                use_container_width=True,
            )

        # Draft expander after invention description (reference)
        with st.expander("ç”Ÿæˆã•ã‚ŒãŸæ˜ç´°æ›¸ãƒ‰ãƒ©ãƒ•ãƒˆï¼ˆå‚è€ƒï¼‰", expanded=False):
            st.markdown(idea.draft_spec_markdown or "æœªç”Ÿæˆ", unsafe_allow_html=False)
            st.markdown("---")
            st.caption("â€» ãƒ‰ãƒ©ãƒ•ãƒˆã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆç¾åœ¨ã®çŠ¶æ…‹ï¼‰")
            c1, c2 = st.columns(2)
            name_docx, data_docx = export_docx(idea.title, idea.draft_spec_markdown)
            c1.download_button(
                "Word ã‚’ä¿å­˜",
                data=data_docx,
                file_name=name_docx,
                mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                use_container_width=True,
            )
            name_pdf, data_pdf = export_pdf(idea.title, idea.draft_spec_markdown)
            c2.download_button(
                "PDF ã‚’ä¿å­˜",
                data=data_pdf,
                file_name=name_pdf,
                mime="application/pdf",
                use_container_width=True,
            )
        st.divider()
        _render_refine_ui(idea)


def main():
    load_dotenv()
    st.set_page_config(page_title=APP_TITLE, layout="wide")
    # Basic authentication gate (enabled via env: BASIC_AUTH_USERNAME/PASSWORD)
    render_login_gate(st)
    st.title(APP_TITLE)
    st.caption("ç‰¹è¨±å‡ºé¡˜ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’å¯¾è©±ã§å…·ä½“åŒ–ã—ã€æ˜ç´°æ›¸è‰æ¡ˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚")

    # Dark theme note: instruct Streamlit to use base dark theme via config.toml if desired

    init_session_state()
    # Sidebar user info + logout when auth enabled
    render_sidebar_user(st)
    sidebar_ui()

    ideas: List[Idea] = st.session_state.ideas
    state: AppState = st.session_state.app_state

    # Main area
    if state.show_new_idea_form and state.selected_idea_id:
        # edit selected
        idea = get_idea(ideas, state.selected_idea_id)
        if idea:
            edit_idea_form(idea)
    elif state.show_new_idea_form:
        new_idea_form()
    else:
        # default: list or selected idea view
        if state.selected_idea_id:
            idea = get_idea(ideas, state.selected_idea_id)
            if idea:
                st.header(idea.title)
                st.markdown(f"**ã‚«ãƒ†ã‚´ãƒª:** {idea.category}")
                st.markdown(f"**æ¦‚è¦:** {idea.description}")
                if st.button("å¯¾è©±é–‹å§‹ / ç¶šãã‹ã‚‰"):
                    st.session_state.start_hearing = True
                if st.session_state.get("start_hearing"):
                    hearing_ui(idea)
            else:
                st.info("ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
        else:
            st.info("å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’é¸æŠã™ã‚‹ã‹ã€æ–°è¦ä½œæˆã—ã¦ãã ã•ã„ã€‚")


if __name__ == "__main__":
    main()
